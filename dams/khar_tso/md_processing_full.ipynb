{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f618bba3-9343-41cc-9492-563c7bcdd580",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Full processing for moraine dam displacement time series\n",
    "Quinn Brencher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0df0d836-9653-4584-96c2-4ce2b2cfff33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "from glob import glob\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from typing import List, Union\n",
    "from osgeo import gdal, gdal_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "from mintpy.utils import readfile, writefile, utils as ut, plot\n",
    "from mintpy.cli import view, tsview, plot_network, plot_transection\n",
    "from mintpy.view import prep_slice, plot_slice\n",
    "import gc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d2fd74-1f33-492d-b30e-58cffba4e1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up paths, directories, and variables for this specific dam\n",
    "data_path = '/home/jovyan/repos/incubator_moraine_dam_analysis/data'\n",
    "asc_burst = '012_023788_IW3'\n",
    "des_burst = '048_101863_IW3'\n",
    "asc_burst_path = f'{data_path}/data_igrams/{asc_burst}'\n",
    "des_burst_path = f'{data_path}/data_igrams/{des_burst}'\n",
    "\n",
    "dam_name = 'khar_tso'\n",
    "proc_path = f'/home/jovyan/repos/incubator_moraine_dam_analysis/dams/{dam_name}'\n",
    "\n",
    "# define reference point, [y, x]\n",
    "reference_point = [3071430.4, 463375.1]\n",
    "\n",
    "# define crs \n",
    "crs = 32645"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa364eb9-9963-4131-a512-a3b6d5a67f45",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e1b112-d518-441e-afb1-5f5e54716661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read in dam polygon\n",
    "dam_fn = f'../../mapping/polygons/{dam_name}_md.shp'\n",
    "dam_gdf = gpd.read_file(dam_fn)\n",
    "dam_gdf = dam_gdf.to_crs(crs) # reproject in case of mistake\n",
    "\n",
    "# # load in moving area polygon\n",
    "# moving_fn = f'../../mapping/polygons/{dam_name}_moving.shp'\n",
    "# moving_gdf = gpd.read_file(moving_fn)\n",
    "\n",
    "# pad dam bounds by 5 km \n",
    "padding = 5000\n",
    "aoi_extent = [dam_gdf.bounds.minx.item()-padding,\n",
    "               dam_gdf.bounds.maxy.item()+padding,\n",
    "               dam_gdf.bounds.maxx.item()+padding,\n",
    "               dam_gdf.bounds.miny.item()-padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4a9131-633d-4789-8d32-3d9edebd7fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clip_hyp3_products_to_common_overlap(data_path: Union[str, Path], overlap: List[float]) -> None:    \n",
    "    files_for_mintpy = ['_rng_off.tif', '_snr.tif', '_conncomp.tif', '_corr.tif', '_unw_phase.tif', '_dem.tif', '_lv_theta.tif', '_lv_phi.tif']\n",
    "    for extension in files_for_mintpy:\n",
    "        print(f'working on {extension}') \n",
    "        for file in data_path.rglob(f'*{extension}'):\n",
    "            dst_file = file.parent / f'{file.stem}_{dam_name}{file.suffix}'\n",
    "            gdal.Translate(destName=str(dst_file), srcDS=str(file), projWin=overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1bbc9d-5805-4db1-b2ac-a6a50679aa8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on _rng_off.tif\n",
      "working on _snr.tif\n",
      "working on _conncomp.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/mintpy/lib/python3.11/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clip igram products to dam aoi\n",
    "clip_hyp3_products_to_common_overlap(Path(asc_burst_path), aoi_extent)\n",
    "clip_hyp3_products_to_common_overlap(Path(des_burst_path), aoi_extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f78cb7-c732-4a09-9c83-68258ee353f3",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Examine coherence time series for dam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee46016-abb5-446f-9ff6-c95d4359dc6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions to load interferogram tifs to xarray\n",
    "def xr_read_geotif(geotif_file_path, masked=True):\n",
    "    \"\"\"\n",
    "    Reads in single or multi-band GeoTIFF as dask array.\n",
    "    Inputs\n",
    "    ----------\n",
    "    GeoTIFF_file_path : GeoTIFF file path\n",
    "    Returns\n",
    "    -------\n",
    "    ds : xarray.Dataset\n",
    "        Includes rioxarray extension to xarray.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    da = rioxarray.open_rasterio(geotif_file_path, masked=True)\n",
    "\n",
    "    # Extract bands and assign as variables in xr.Dataset()\n",
    "    ds = xr.Dataset()\n",
    "    for i, v in enumerate(da.band):\n",
    "        da_tmp = da.sel(band=v)\n",
    "        da_tmp.name = \"band\" + str(i + 1)\n",
    "\n",
    "        ds[da_tmp.name] = da_tmp\n",
    "\n",
    "    # Delete empty band coordinates.\n",
    "    # Need to preserve spatial_ref coordinate, even though it appears empty.\n",
    "    # See spatial_ref attributes under ds.coords.variables used by rioxarray extension.\n",
    "    del ds.coords[\"band\"]\n",
    "\n",
    "    # Preserve top-level attributes and extract single value from value iterables e.g. (1,) --> 1\n",
    "    ds.attrs = da.attrs\n",
    "    for key, value in ds.attrs.items():\n",
    "        try:\n",
    "            if len(value) == 1:\n",
    "                ds.attrs[key] = value[0]\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "    return ds\n",
    "\n",
    "def hyp3_to_xarray(hyp3_dir, file_type='corr', epsg=32645, interpolate_na=False):\n",
    "    \n",
    "    dirs = os.listdir(hyp3_dir) #list generated interferograms\n",
    "    datasets = []\n",
    "    acquisition_list = []\n",
    "    \n",
    "    for idir in dirs:\n",
    "        tif_path = glob(f'{hyp3_dir}/{idir}/*/*{file_type}.tif')[0]\n",
    "        tif_fn = os.path.split(tif_path)[-1]\n",
    "        dates = f'{tif_fn[14:22]}_{tif_fn[23:31]}' #parse filename for interferogram dates\n",
    "        start_date = datetime.strptime(dates[:8], '%Y%m%d')\n",
    "        end_date = datetime.strptime(dates[-8:], '%Y%m%d')\n",
    "        t_baseline = end_date - start_date\n",
    "        \n",
    "        src = xr_read_geotif(tif_path, masked=False) #read product to xarray ds\n",
    "        src = src.assign_coords({\"dates\": dates})\n",
    "        src = src.expand_dims(\"dates\")\n",
    "        \n",
    "        src = src.assign_coords(start_date = ('dates', [start_date]))\n",
    "        src = src.assign_coords(end_date = ('dates', [end_date]))\n",
    "        src = src.assign_coords(t_baseline = ('dates', [t_baseline]))\n",
    "        \n",
    "        src = src.rename({'band1':file_type})\n",
    "        src = src.rio.write_crs(epsg)\n",
    "        # in future, should get spatial baseline, flight dir, and orbit from metadata\n",
    "        \n",
    "        if interpolate_na == True:\n",
    "            src = src.interpolate_na(dim='x', method='linear').fillna(value=0)\n",
    "        \n",
    "        datasets.append(src)\n",
    "        #print(src[file_type].shape, src.dates.item())\n",
    "       \n",
    "    ds = xr.concat(datasets, dim=\"dates\", combine_attrs=\"no_conflicts\") #create dataset\n",
    "    ds = ds.sortby('dates')\n",
    "\n",
    "    return ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153b2f6-a036-4ee8-8000-0f66c8f447ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asc_corr_ds = hyp3_to_xarray(asc_burst_path, file_type=f'corr_{dam_name}')\n",
    "des_corr_ds = hyp3_to_xarray(des_burst_path, file_type=f'corr_{dam_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa0faf-847e-4a5b-8b38-bfb4838c2d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot mean spatial coherence\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "asc_corr_ds[f'corr_{dam_name}'].mean(dim='dates').plot(ax=ax[0], vmin=0.2, vmax=1, cbar_kwargs= {'label':'coherence'})\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].set_title('ascending mean coherence')\n",
    "ax[0].scatter(reference_point[1], reference_point[0], c='k', s=5)\n",
    "ax[0].set_aspect('equal')\n",
    "# ax[0].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "# ax[0].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "des_corr_ds[f'corr_{dam_name}'].mean(dim='dates').plot(ax=ax[1], vmin=0.2, vmax=1, cbar_kwargs= {'label':'coherence'})\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[1].scatter(reference_point[1], reference_point[0], c='k', s=5)\n",
    "ax[1].set_title('descending mean coherence')\n",
    "ax[1].set_aspect('equal')\n",
    "# ax[1].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "# ax[1].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "f.tight_layout()\n",
    "plt.savefig(f'./figs/{dam_name}_coherence_map.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704ce1c-4f08-4c8c-8715-843744bc2b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clip datasets to dam polygon\n",
    "asc_dam_corr_ds = asc_corr_ds.rio.clip(dam_gdf.geometry.values, crs=dam_gdf.crs, drop=True)\n",
    "asc_dam_corr_ds = asc_dam_corr_ds.assign_coords(middle_date = ('dates', (asc_dam_corr_ds.t_baseline/2+asc_dam_corr_ds.start_date).data))\n",
    "\n",
    "des_dam_corr_ds = des_corr_ds.rio.clip(dam_gdf.geometry.values, crs=dam_gdf.crs, drop=True)\n",
    "des_dam_corr_ds = des_dam_corr_ds.assign_coords(middle_date = ('dates', (des_dam_corr_ds.t_baseline/2+des_dam_corr_ds.start_date).data))\n",
    "\n",
    "# # clip datasets to moving polygon\n",
    "# asc_dam_corr_ds = asc_corr_ds.rio.clip(moving_gdf.geometry.values, crs=moving_gdf.crs, drop=True)\n",
    "# asc_dam_corr_ds = asc_dam_corr_ds.assign_coords(middle_date = ('dates', (asc_dam_corr_ds.t_baseline/2+asc_dam_corr_ds.start_date).data))\n",
    "\n",
    "# des_dam_corr_ds = des_corr_ds.rio.clip(moving_gdf.geometry.values, crs=moving_gdf.crs, drop=True)\n",
    "# des_dam_corr_ds = des_dam_corr_ds.assign_coords(middle_date = ('dates', (des_dam_corr_ds.t_baseline/2+des_dam_corr_ds.start_date).data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392566e1-5a32-4c8f-adc5-72bf8302e794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_baselines = np.union1d(des_dam_corr_ds.t_baseline.dt.days.values, asc_dam_corr_ds.t_baseline.dt.days.values)\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "# plot seasonal changes in coherence \n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(t_baselines)))\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "for i, days in enumerate(t_baselines):\n",
    "    ax[0].scatter(asc_dam_corr_ds.middle_date.dt.dayofyear[asc_dam_corr_ds.t_baseline.dt.days == days],\n",
    "                  asc_dam_corr_ds[f'corr_{dam_name}'].mean(dim=['x', 'y'])[asc_dam_corr_ds.t_baseline.dt.days == days],\n",
    "                  color=colors[i])\n",
    "    \n",
    "    ax[1].scatter(des_dam_corr_ds.middle_date.dt.dayofyear[des_dam_corr_ds.t_baseline.dt.days == days],\n",
    "                  des_dam_corr_ds[f'corr_{dam_name}'].mean(dim=['x', 'y'])[des_dam_corr_ds.t_baseline.dt.days == days],\n",
    "                  color=colors[i],\n",
    "                  label=f'{days} days')\n",
    "    \n",
    "ax[0].set_xlabel('day of year of interferogram midpoint')\n",
    "ax[0].set_ylabel('spatial mean coherence')\n",
    "ax[0].set_title('ascending pass dam coherence')\n",
    "\n",
    "ax[1].set_xlabel('day of year of interferogram midpoint')\n",
    "ax[1].set_title('descending pass dam coherence')\n",
    "ax[1].legend(loc='lower right')\n",
    "f.tight_layout()\n",
    "plt.savefig(f'./figs/{dam_name}_coherence_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb7f91-865b-41ca-b216-8a72ec9b9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret plot\n",
    "# if offsets will be needed to bridge summer gap, set use_offsets = True\n",
    "use_offsets = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297d5e7-b4b0-4a41-8227-21e1d2fe7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_acquisitions = xr.DataArray(np.union1d(asc_corr_ds.start_date, asc_corr_ds.end_date))\n",
    "des_acquisitions = xr.DataArray(np.union1d(des_corr_ds.start_date, des_corr_ds.end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ea9e77-f28a-4746-a463-d7b0c18a3f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_acquisitions(da, min_doy=210, max_doy=300):\n",
    "    da_bad = da.where(da.dt.dayofyear > min_doy, drop=True)\n",
    "    da_bad = da_bad.where(da_bad.dt.dayofyear < max_doy, drop=True)\n",
    "    date_string = ''\n",
    "    # clunky, but need set to remove duplicates, then sort list to prevent random iteration order\n",
    "    for date in sorted(list(set(da_bad.dt.strftime('%Y%m%d').values))): \n",
    "        date_string += f'{date},'   \n",
    "    date_string = date_string[:-1]\n",
    "    return date_string, da_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b54ecf-cd17-45c7-92b7-3794f8e8c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab dates to remove if use_offsets is true\n",
    "if use_offsets == True:\n",
    "    asc_bad_acquisitions, asc_da_bad = get_bad_acquisitions(asc_acquisitions, min_doy=0, max_doy=150)\n",
    "    des_bad_acquisitions, des_da_bad = get_bad_acquisitions(des_acquisitions, min_doy=0, max_doy=150)\n",
    "else:\n",
    "    asc_bad_acquisitions = 'auto'\n",
    "    des_bad_acquisitions = 'auto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0356cd-1822-499e-95eb-0afde97950df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # close datasets to free memory \n",
    "# asc_corr_ds.close()\n",
    "# des_corr_ds.close()\n",
    "# asc_dam_corr_ds.close()\n",
    "# des_dam_corr_ds.close()\n",
    "\n",
    "# del asc_corr_ds\n",
    "# del des_corr_ds\n",
    "# del asc_dam_corr_ds\n",
    "# del des_dam_corr_ds\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca92bfd-93bd-4358-a6d0-76e7d3b9cb71",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Calculate offset stacks to bridge gaps\n",
    "only need this if there's a seasonal drop in coherence that can't be bridged with an interferogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da48c4-ab1b-4a53-8b19-9224df440ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "asc_burst_offsets_path = f'{data_path}/data_offsets/{asc_burst}'\n",
    "des_burst_offsets_path = f'{data_path}/data_offsets/{des_burst}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254159a-afaa-4d6f-8655-d027fb25807c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # clip offset products to dam aoi\n",
    "# clip_hyp3_products_to_common_overlap(Path(asc_burst_offsets_path), aoi_extent)\n",
    "# clip_hyp3_products_to_common_overlap(Path(des_burst_offsets_path), aoi_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9a2ba-d1c9-4ceb-9d58-8f7a4624a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_offset_stack(ds, dam_name, crs, start_date = None, end_date = None, mean = True):\n",
    "    # convert from pixels to phase\n",
    "    ds['rng_off_radians'] = (ds[f'rng_off_{dam_name}']*2.3)*(12.5663706/0.05546576)\n",
    "    da = xr.where(ds.rng_off_radians < -5000000, np.nan, ds.rng_off_radians)\n",
    "\n",
    "    if start_date == None:\n",
    "        start_date = da.start_date[0]\n",
    "    if end_date == None:\n",
    "        end_date = da.end_date[-1]\n",
    "    full_t_baseline = end_date - start_date \n",
    "\n",
    "    # calculate mean displacement\n",
    "    if mean == True:\n",
    "        ones = xr.ones_like(da)\n",
    "        ones_masked = xr.where(da.isnull(), 0, ones)\n",
    "        t_baseline_broadcast = ones_masked*da.t_baseline.dt.days\n",
    "        \n",
    "        mean_offset_per_day = da.sum(dim='dates')/t_baseline_broadcast.sum(dim='dates')\n",
    "    \n",
    "        # scale to full temporal baseline\n",
    "        full_displacement = mean_offset_per_day*full_t_baseline.dt.days\n",
    "\n",
    "    # calculate median displacement\n",
    "    else:\n",
    "        median_offset_per_day = (da/da.t_baseline.dt.days).median(dim='dates')\n",
    "        full_displacement = median_offset_per_day*full_t_baseline.dt.days\n",
    "    \n",
    "    # write crs, nodata, convert type\n",
    "    full_displacement = full_displacement.astype(np.float32)\n",
    "    full_displacement = full_displacement.rio.write_nodata(0, inplace=True)\n",
    "    full_displacement = full_displacement.rio.write_crs(crs)\n",
    "    \n",
    "    return full_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf46f1-e7e5-4c28-8de9-474ab954813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open offsets\n",
    "asc_offset_ds = hyp3_to_xarray(asc_burst_offsets_path, file_type=f'rng_off_{dam_name}')\n",
    "des_offset_ds = hyp3_to_xarray(des_burst_offsets_path, file_type=f'rng_off_{dam_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bdce5-250a-4a1a-9f07-a4e28fb28018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find start and end date for offset stack\n",
    "asc_corr_ds_high = asc_corr_ds.sel(dates = ~asc_corr_ds.start_date.isin(asc_da_bad) & ~asc_corr_ds.end_date.isin(asc_da_bad))\n",
    "asc_start_date = asc_corr_ds_high.start_date[0]\n",
    "asc_end_date = asc_corr_ds_high.end_date[-1]\n",
    "\n",
    "des_corr_ds_high = des_corr_ds.sel(dates = ~des_corr_ds.start_date.isin(des_da_bad) & ~des_corr_ds.end_date.isin(des_da_bad))\n",
    "des_start_date = des_corr_ds_high.start_date[0]\n",
    "des_end_date = des_corr_ds_high.end_date[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11c41f-999a-4311-85d2-9353046ab1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_stack = calculate_offset_stack(asc_offset_ds, dam_name, crs, asc_start_date, asc_end_date, mean=False)\n",
    "des_stack = calculate_offset_stack(des_offset_ds, dam_name, crs, des_start_date, des_end_date, mean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e1ef2-cc61-407d-8f7f-cb6c666f179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n",
    "asc_stack.plot(ax=ax[0], vmin=-500, vmax=500, cmap='RdBu_r')\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].scatter(reference_point[1], reference_point[0], c='k', s=5)\n",
    "ax[0].set_title('ascending stack (radians)')\n",
    "des_stack.plot(ax=ax[1], vmin=-500, vmax=500, cmap='RdBu_r')\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[1].scatter(reference_point[1], reference_point[0], c='k', s=5)\n",
    "ax[1].set_title('descending stack (radians)')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7e126-aa3e-4302-aaca-5818ea519f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write to text file\n",
    "def write_config_file(out_file, CONFIG_TXT, mode='a'): \n",
    "    \"\"\"Write configuration files for MintPy to process products\"\"\"\n",
    "    if not os.path.isfile(out_file) or mode == 'w':\n",
    "        with open(out_file, \"w\") as fid:\n",
    "            fid.write(CONFIG_TXT)\n",
    "        print('write configuration to file: {}'.format(out_file))\n",
    "    else:\n",
    "        with open(out_file, \"a\") as fid:\n",
    "            fid.write(\"\\n\" + CONFIG_TXT)\n",
    "        print('add the following to file: \\n{}'.format(CONFIG_TXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c253d74f-0147-4e9e-ad4a-07aa08bc41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to rasters\n",
    "os.makedirs(f'{data_path}/data_stacks/{asc_burst}', exist_ok=True)\n",
    "date1 = asc_start_date.dt.strftime('%Y%m%d').item()\n",
    "date2 = asc_end_date.dt.strftime('%Y%m%d').item()\n",
    "os.makedirs(f'{data_path}/data_stacks/{asc_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/', exist_ok=True)\n",
    "\n",
    "asc_stack.rio.to_raster(f'{data_path}/data_stacks/{asc_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_rng_off_unw_phase_{dam_name}.tif')\n",
    "\n",
    "# create phony coherence and connected components\n",
    "ones = xr.ones_like(asc_stack)\n",
    "ones.rio.to_raster(f'{data_path}/data_stacks/{asc_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_phonycorr_{dam_name}.tif')\n",
    "ones.rio.to_raster(f'{data_path}/data_stacks/{asc_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_phonyconncomp_{dam_name}.tif')\n",
    "\n",
    "META_TXT = f'''Reference Granule: S1A_IW_SLC__1SSV_{date1}T121323_{date1}T121353_002884_00343C_D259\n",
    "Secondary Granule: S1A_IW_SLC__1SSV_{date1}T121312_{date1}T121347_003059_0037F7_098A\n",
    "Reference Pass Direction: ASCENDING\n",
    "Reference Orbit Number: 2884\n",
    "Secondary Pass Direction: ASCENDING\n",
    "Secondary Orbit Number: 3059\n",
    "Baseline: 0\n",
    "UTC time: 1523.843923\n",
    "Heading: -12.55120412179744\n",
    "Spacecraft height: 693000.0\n",
    "Earth radius at nadir: 6337286.638938101\n",
    "Slant range near: 799643.0637567625\n",
    "Slant range center: 824772.0502881967\n",
    "Slant range far: 849901.0368196309\n",
    "Range looks: 5\n",
    "Azimuth looks: 1\n",
    "INSAR phase filter: yes\n",
    "Phase filter parameter: 0.5\n",
    "Range bandpass filter: no\n",
    "Azimuth bandpass filter: no\n",
    "DEM source: GLO_30\n",
    "DEM resolution (m): 30\n",
    "Unwrapping type: snaphu_mcf\n",
    "Speckle filter: yes\n",
    "Water mask: False'''\n",
    "\n",
    "write_config_file(f'{data_path}/data_stacks/{asc_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS.txt', META_TXT, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b762df-5569-4f8b-94a2-f6b7114b6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out to rasters\n",
    "os.makedirs(f'{data_path}/data_stacks/{des_burst}', exist_ok=True)\n",
    "date1 = des_start_date.dt.strftime('%Y%m%d').item()\n",
    "date2 = des_end_date.dt.strftime('%Y%m%d').item()\n",
    "os.makedirs(f'{data_path}/data_stacks/{des_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/', exist_ok=True)\n",
    "\n",
    "des_stack.rio.to_raster(f'{data_path}/data_stacks/{des_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_rng_off_unw_phase_{dam_name}.tif')\n",
    "\n",
    "# create phony coherence and connected components\n",
    "ones = xr.ones_like(des_stack)\n",
    "ones.rio.to_raster(f'{data_path}/data_stacks/{des_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_phonycorr_{dam_name}.tif')\n",
    "ones.rio.to_raster(f'{data_path}/data_stacks/{des_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS_phonyconncomp_{dam_name}.tif')\n",
    "\n",
    "# fake metafile, none of this should actually matter\n",
    "META_TXT = f'''Reference Granule: S1A_IW_SLC__1SSV_{date1}T121323_{date1}T121353_002884_00343C_D259\n",
    "Secondary Granule: S1A_IW_SLC__1SSV_{date2}T121312_{date2}T121347_003059_0037F7_098A\n",
    "Reference Pass Direction: DESCENDING\n",
    "Reference Orbit Number: 2884\n",
    "Secondary Pass Direction: DESCENDING\n",
    "Secondary Orbit Number: 3059\n",
    "Baseline: 0\n",
    "UTC time: 1523.843923\n",
    "Heading: -12.55120412179744\n",
    "Spacecraft height: 693000.0\n",
    "Earth radius at nadir: 6337286.638938101\n",
    "Slant range near: 799643.0637567625\n",
    "Slant range center: 824772.0502881967\n",
    "Slant range far: 849901.0368196309\n",
    "Range looks: 5\n",
    "Azimuth looks: 1\n",
    "INSAR phase filter: yes\n",
    "Phase filter parameter: 0.5\n",
    "Range bandpass filter: no\n",
    "Azimuth bandpass filter: no\n",
    "DEM source: GLO_30\n",
    "DEM resolution (m): 30\n",
    "Unwrapping type: snaphu_mcf\n",
    "Speckle filter: yes\n",
    "Water mask: False'''\n",
    "\n",
    "write_config_file(f'{data_path}/data_stacks/{des_burst}/{date1}_{date2}/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS/S1_258661_IW2_{date1}_{date2}_VV_INT20_FFTS.txt', META_TXT, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d4052-6f44-47c0-bcfa-301373718bc6",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## run mintpy time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a70324-8672-4e6a-b39a-18a076767cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mintpy(mintpy_path,\n",
    "               data_dir,\n",
    "               burst_name,\n",
    "               dam_name,\n",
    "               reference_point, \n",
    "               dam_gdf,\n",
    "               use_offsets=False, \n",
    "               drop_acquisitions='auto'):\n",
    "    \n",
    "    if use_offsets == True:\n",
    "        data_path = f'{data_dir}/data*/{burst_name}/*/*'\n",
    "    else:\n",
    "        data_path = f'{data_dir}/data_igrams/{burst_name}/*/*'      \n",
    "    \n",
    "    # write config text \n",
    "    CONFIG_TXT = f'''\n",
    "    # vim: set filetype=cfg:\n",
    "    ##------------------------ smallbaselineApp.cfg ------------------------##\n",
    "    ########## computing resource configuration\n",
    "    mintpy.compute.maxMemory = 30 #auto for 4, max memory to allocate in GB\n",
    "    ## parallel processing with dask\n",
    "    ## currently apply to steps: invert_network, correct_topography\n",
    "    ## cluster   = none to turn off the parallel computing\n",
    "    ## numWorker = all  to use all of locally available cores (for cluster = local only)\n",
    "    ## numWorker = 80%  to use 80% of locally available cores (for cluster = local only)\n",
    "    ## config    = none to rollback to the default name (same as the cluster type; for cluster != local)\n",
    "    mintpy.compute.cluster   = local #[local / slurm / pbs / lsf / none], auto for none, cluster type\n",
    "    mintpy.compute.numWorker = 8 #[int > 1 / all / num%], auto for 4 (local) or 40 (slurm / pbs / lsf), num of workers\n",
    "    mintpy.compute.config    = auto #[none / slurm / pbs / lsf ], auto for none (same as cluster), config name\n",
    "\n",
    "\n",
    "    ########## 1. load_data\n",
    "    ##---------add attributes manually\n",
    "    ## MintPy requires attributes listed at: https://mintpy.readthedocs.io/en/latest/api/attributes/\n",
    "    ## Missing attributes can be added below manually (uncomment #), e.g.\n",
    "    # ORBIT_DIRECTION = ascending\n",
    "    # PLATFORM = Sen\n",
    "    # ...\n",
    "    ## a. autoPath - automatic path pattern defined in mintpy.defaults.auto_path.AUTO_PATH_*\n",
    "    ## b. load_data.py -H to check more details and example inputs.\n",
    "    ## c. compression to save disk usage for ifgramStack.h5 file:\n",
    "    ## no   - save   0% disk usage, fast [default]\n",
    "    ## lzf  - save ~57% disk usage, relative slow\n",
    "    ## gzip - save ~62% disk usage, very slow [not recommend]\n",
    "    mintpy.load.processor       = hyp3 #[isce, aria, hyp3, gmtsar, snap, gamma, roipac, nisar], auto for isce\n",
    "    mintpy.load.autoPath        = auto  #[yes / no], auto for no, use pre-defined auto path\n",
    "    mintpy.load.updateMode      = auto  #[yes / no], auto for yes, skip re-loading if HDF5 files are complete\n",
    "    mintpy.load.compression     = auto  #[gzip / lzf / no], auto for no.\n",
    "    ##---------interferogram stack:\n",
    "    mintpy.load.unwFile         = {data_path}/S1*unw_phase*{dam_name}.tif  #[path pattern of unwrapped interferogram files]\n",
    "    mintpy.load.corFile         = {data_path}/S1*corr_{dam_name}.tif  #[path pattern of spatial coherence files]\n",
    "    mintpy.load.connCompFile    = {data_path}/S1*conncomp_{dam_name}.tif  #[path pattern of connected components files], optional but recommended\n",
    "    ##---------geometry:\n",
    "    mintpy.load.demFile         = {data_path}/S1*dem_{dam_name}.tif   #[path of DEM file]\n",
    "    mintpy.load.incAngleFile    = {data_path}/S1*lv_theta_{dam_name}.tif   #[path of incidence angle file], optional but recommended\n",
    "    mintpy.load.azAngleFile     = {data_path}/S1*lv_phi_{dam_name}.tif   #[path of azimuth   angle file], optional\n",
    "    ########## 2. modify_network\n",
    "    ## 1) Network modification based on temporal/perpendicular baselines, date, num of connections etc.\n",
    "    mintpy.network.tempBaseMax     = auto  #[1-inf, no], auto for no, max temporal baseline in days\n",
    "    mintpy.network.perpBaseMax     = auto  #[1-inf, no], auto for no, max perpendicular spatial baseline in meter\n",
    "    mintpy.network.connNumMax      = auto  #[1-inf, no], auto for no, max number of neighbors for each acquisition\n",
    "    mintpy.network.startDate       = auto  #[20090101 / no], auto for no\n",
    "    mintpy.network.endDate         = auto #20240124 #[20110101 / no], auto for no\n",
    "    mintpy.network.excludeDate     = {drop_acquisitions}  #[20080520,20090817 / no], auto for no\n",
    "    mintpy.network.excludeIfgIndex = auto  #[1:5,25 / no], auto for no, list of ifg index (start from 0)\n",
    "    mintpy.network.referenceFile   = auto  #[date12_list.txt / ifgramStack.h5 / no], auto for no\n",
    "\n",
    "    ## 2) Data-driven network modification\n",
    "    ## a - Coherence-based network modification = (threshold + MST) by default\n",
    "    ## reference: Yunjun et al. (2019, section 4.2 and 5.3.1); Chaussard et al. (2015, GRL)\n",
    "    ## It calculates a average coherence for each interferogram using spatial coherence based on input mask (with AOI)\n",
    "    ## Then it finds a minimum spanning tree (MST) network with inverse of average coherence as weight (keepMinSpanTree)\n",
    "    ## Next it excludes interferograms if a) the average coherence < minCoherence AND b) not in the MST network.\n",
    "    mintpy.network.coherenceBased  = yes  #[yes / no], auto for no, exclude interferograms with coherence < minCoherence\n",
    "    mintpy.network.minCoherence    = 0.55  #[0.0-1.0], auto for 0.7\n",
    "\n",
    "    ## b - Effective Coherence Ratio network modification = (threshold + MST) by default\n",
    "    ## reference: Kang et al. (2021, RSE)\n",
    "    ## It calculates the area ratio of each interferogram that is above a spatial coherence threshold.\n",
    "    ## This threshold is defined as the spatial coherence of the interferograms within the input mask.\n",
    "    ## It then finds a minimum spanning tree (MST) network with inverse of the area ratio as weight (keepMinSpanTree)\n",
    "    ## Next it excludes interferograms if a) the area ratio < minAreaRatio AND b) not in the MST network.\n",
    "    mintpy.network.areaRatioBased  = auto  #[yes / no], auto for no, exclude interferograms with area ratio < minAreaRatio\n",
    "    mintpy.network.minAreaRatio    = auto  #[0.0-1.0], auto for 0.75\n",
    "\n",
    "    ## Additional common parameters for the 2) data-driven network modification\n",
    "    mintpy.network.keepMinSpanTree = yes  #[yes / no], auto for yes, keep interferograms in Min Span Tree network\n",
    "    mintpy.network.maskFile        = auto  #[file name, no], auto for waterMask.h5 or no [if no waterMask.h5 found]\n",
    "    mintpy.network.aoiYX           = auto  #[y0:y1,x0:x1 / no], auto for no, area of interest for coherence calculation\n",
    "    mintpy.network.aoiLALO         = {dam_gdf.bounds.miny.item()}:{dam_gdf.bounds.maxy.item()},{dam_gdf.bounds.minx.item()}:{dam_gdf.bounds.maxx.item()} #[S:N,W:E / no], auto for no - use the whole area\n",
    "\n",
    "    ########## 3. reference_point\n",
    "    ## Reference all interferograms to one common point in space\n",
    "    ## auto - randomly select a pixel with coherence > minCoherence\n",
    "    ## however, manually specify using prior knowledge of the study area is highly recommended\n",
    "    ##   with the following guideline (section 4.3 in Yunjun et al., 2019):\n",
    "    ## 1) located in a coherence area, to minimize the decorrelation effect.\n",
    "    ## 2) not affected by strong atmospheric turbulence, i.e. ionospheric streaks\n",
    "    ## 3) close to and with similar elevation as the AOI, to minimize the impact of spatially correlated atmospheric delay\n",
    "    mintpy.reference.yx            = auto   #[257,151 / auto]\n",
    "    mintpy.reference.lalo          = {reference_point[0]}, {reference_point[1]} # y, x, 28.003585, 88.480191  #[31.8,130.8 / auto]\n",
    "    mintpy.reference.maskFile      = no   #[filename / no], auto for maskConnComp.h5\n",
    "    mintpy.reference.coherenceFile = auto   #[filename], auto for avgSpatialCoh.h5\n",
    "    mintpy.reference.minCoherence  = auto   #[0.0-1.0], auto for 0.85, minimum coherence for auto method\n",
    "\n",
    "\n",
    "    ########## quick_overview\n",
    "    ## A quick assessment of:\n",
    "    ## 1) possible groud deformation\n",
    "    ##    using the velocity from the traditional interferogram stacking\n",
    "    ##    reference: Zebker et al. (1997, JGR)\n",
    "    ## 2) distribution of phase unwrapping error\n",
    "    ##    from the number of interferogram triplets with non-zero integer ambiguity of closue phase\n",
    "    ##    reference: T_int in Yunjun et al. (2019, CAGEO). Related to section 3.2, equation (8-9) and Fig. 3d-e.\n",
    "\n",
    "\n",
    "    ########## 4. correct_unwrap_error (optional)\n",
    "    ## connected components (mintpy.load.connCompFile) are required for this step.\n",
    "    ## SNAPHU (Chem & Zebker,2001) is currently the only unwrapper that provides connected components as far as we know.\n",
    "    ## reference: Yunjun et al. (2019, section 3)\n",
    "    ## supported methods:\n",
    "    ## a. phase_closure          - suitable for highly redundant network\n",
    "    ## b. bridging               - suitable for regions separated by narrow decorrelated features, e.g. rivers, narrow water bodies\n",
    "    ## c. bridging+phase_closure - recommended when there is a small percentage of errors left after bridging\n",
    "    mintpy.unwrapError.method          = no #bridging+phase_closure  #[bridging / phase_closure / bridging+phase_closure / no], auto for no\n",
    "    mintpy.unwrapError.waterMaskFile   = auto  #[waterMask.h5 / no], auto for waterMask.h5 or no [if not found]\n",
    "    mintpy.unwrapError.connCompMinArea = 100  #[1-inf], auto for 2.5e3, discard regions smaller than the min size in pixels\n",
    "\n",
    "    ## phase_closure options:\n",
    "    ## numSample - a region-based strategy is implemented to speedup L1-norm regularized least squares inversion.\n",
    "    ##     Instead of inverting every pixel for the integer ambiguity, a common connected component mask is generated,\n",
    "    ##     for each common conn. comp., numSample pixels are radomly selected for inversion, and the median value of the results\n",
    "    ##     are used for all pixels within this common conn. comp.\n",
    "    mintpy.unwrapError.numSample       = auto  #[int>1], auto for 100, number of samples to invert for common conn. comp.\n",
    "\n",
    "    ## bridging options:\n",
    "    ## ramp - a phase ramp could be estimated based on the largest reliable region, removed from the entire interferogram\n",
    "    ##     before estimating the phase difference between reliable regions and added back after the correction.\n",
    "    ## bridgePtsRadius - half size of the window used to calculate the median value of phase difference\n",
    "    mintpy.unwrapError.ramp            = auto  #[linear / quadratic], auto for no; recommend linear for L-band data\n",
    "    mintpy.unwrapError.bridgePtsRadius = auto  #[1-inf], auto for 50, half size of the window around end points\n",
    "\n",
    "\n",
    "    ########## 5. invert_network\n",
    "    ## Invert network of interferograms into time-series using weighted least square (WLS) estimator.\n",
    "    ## weighting options for least square inversion [fast option available but not best]:\n",
    "    ## a. var - use inverse of covariance as weight (Tough et al., 1995; Guarnieri & Tebaldini, 2008) [recommended]\n",
    "    ## b. fim - use Fisher Information Matrix as weight (Seymour & Cumming, 1994; Samiei-Esfahany et al., 2016).\n",
    "    ## c. coh - use coherence as weight (Perissin & Wang, 2012)\n",
    "    ## d. no  - uniform weight (Berardino et al., 2002) [fast]\n",
    "    ## SBAS (Berardino et al., 2002) = minNormVelocity (yes) + weightFunc (no)\n",
    "    mintpy.networkInversion.weightFunc      = var #[var / fim / coh / no], auto for var\n",
    "    mintpy.networkInversion.waterMaskFile   = auto #[filename / no], auto for waterMask.h5 or no [if not found]\n",
    "    mintpy.networkInversion.minNormVelocity = auto #[yes / no], auto for yes, min-norm deformation velocity / phase\n",
    "\n",
    "    ## mask options for unwrapPhase of each interferogram before inversion (recommend if weightFunct=no):\n",
    "    ## a. coherence              - mask out pixels with spatial coherence < maskThreshold\n",
    "    ## b. connectComponent       - mask out pixels with False/0 value\n",
    "    ## c. no                     - no masking [recommended].\n",
    "    ## d. range/azimuthOffsetStd - mask out pixels with offset std. dev. > maskThreshold [for offset]\n",
    "    mintpy.networkInversion.maskDataset   = auto #[coherence / connectComponent / rangeOffsetStd / azimuthOffsetStd / no], auto for no\n",
    "    mintpy.networkInversion.maskThreshold = 0.6 #[0-inf], auto for 0.4\n",
    "    mintpy.networkInversion.minRedundancy = auto #[1-inf], auto for 1.0, min num_ifgram for every SAR acquisition\n",
    "\n",
    "    ## Temporal coherence is calculated and used to generate the mask as the reliability measure\n",
    "    ## reference: Pepe & Lanari (2006, IEEE-TGRS)\n",
    "    mintpy.networkInversion.minTempCoh  = 0.0 #[0.0-1.0], auto for 0.7, min temporal coherence for mask\n",
    "    mintpy.networkInversion.minNumPixel = auto #[int > 1], auto for 100, min number of pixels in mask above\n",
    "    mintpy.networkInversion.shadowMask  = auto #[yes / no], auto for yes [if shadowMask is in geometry file] or no.\n",
    "\n",
    "    ########## 6. correct_troposphere (optional but recommended)\n",
    "    ## correct tropospheric delay using the following methods:\n",
    "    ## a. height_correlation - correct stratified tropospheric delay (Doin et al., 2009, J Applied Geop)\n",
    "    ## b. pyaps - use Global Atmospheric Models (GAMs) data (Jolivet et al., 2011; 2014)\n",
    "    ##      ERA5  - ERA5    from ECMWF [need to install PyAPS from GitHub; recommended and turn ON by default]\n",
    "    ##      MERRA - MERRA-2 from NASA  [need to install PyAPS from Caltech/EarthDef]\n",
    "    ##      NARR  - NARR    from NOAA  [need to install PyAPS from Caltech/EarthDef; recommended for N America]\n",
    "    ## c. gacos - use GACOS with the iterative tropospheric decomposition model (Yu et al., 2018, JGR)\n",
    "    ##      need to manually download GACOS products at http://www.gacos.net for all acquisitions before running this step\n",
    "    mintpy.troposphericDelay.method = no  #[pyaps / height_correlation / gacos / no], auto for pyaps\n",
    "\n",
    "    ## Notes for pyaps:\n",
    "    ## a. GAM data latency: with the most recent SAR data, there will be GAM data missing, the correction\n",
    "    ##    will be applied to dates with GAM data available and skipped for the others.\n",
    "    ## b. WEATHER_DIR: if you define an environment variable named WEATHER_DIR to contain the path to a\n",
    "    ##    directory, then MintPy applications will download the GAM files into the indicated directory.\n",
    "    ##    MintPy application will look for the GAM files in the directory before downloading a new one to\n",
    "    ##    prevent downloading multiple copies if you work with different dataset that cover the same date/time.\n",
    "    mintpy.troposphericDelay.weatherModel = auto  #[ERA5 / MERRA / NARR], auto for ERA5\n",
    "    mintpy.troposphericDelay.weatherDir   = auto  #[path2directory], auto for WEATHER_DIR or \"./\"\n",
    "\n",
    "    ## Notes for height_correlation:\n",
    "    ## Extra multilooking is applied to estimate the empirical phase/elevation ratio ONLY.\n",
    "    ## For an dataset with 5 by 15 looks, looks=8 will generate phase with (5*8) by (15*8) looks\n",
    "    ## to estimate the empirical parameter; then apply the correction to original phase (with 5 by 15 looks),\n",
    "    ## if the phase/elevation correlation is larger than minCorrelation.\n",
    "    mintpy.troposphericDelay.polyOrder      = auto  #[1 / 2 / 3], auto for 1\n",
    "    mintpy.troposphericDelay.looks          = auto  #[1-inf], auto for 8, extra multilooking num\n",
    "    mintpy.troposphericDelay.minCorrelation = auto  #[0.0-1.0], auto for 0\n",
    "\n",
    "    ## Notes for gacos:\n",
    "    ## Set the path below to directory that contains the downloaded *.ztd* files\n",
    "    mintpy.troposphericDelay.gacosDir = auto # [path2directory], auto for \"./GACOS\"\n",
    "\n",
    "\n",
    "    ########## 7. deramp (optional)\n",
    "    ## Estimate and remove a phase ramp for each acquisition based on the reliable pixels.\n",
    "    ## Recommended for localized deformation signals, i.e. volcanic deformation, landslide and land subsidence, etc.\n",
    "    ## NOT recommended for long spatial wavelength deformation signals, i.e. co-, post- and inter-seimic deformation.\n",
    "    mintpy.deramp          = no  #[no / linear / quadratic], auto for no - no ramp will be removed\n",
    "    mintpy.deramp.maskFile = auto  #[filename / no], auto for maskTempCoh.h5, mask file for ramp estimation\n",
    "\n",
    "\n",
    "    ########## 8. correct_topography (optional but recommended)\n",
    "    ## Topographic residual (DEM error) correction\n",
    "    ## reference: Fattahi and Amelung (2013, IEEE-TGRS)\n",
    "    ## stepFuncDate      - specify stepFuncDate option if you know there are sudden displacement jump in your area,\n",
    "    ##                     e.g. volcanic eruption, or earthquake\n",
    "    ## excludeDate       - dates excluded for the error estimation\n",
    "    ## pixelwiseGeometry - use pixel-wise geometry (incidence angle & slant range distance)\n",
    "    ##                     yes - use pixel-wise geometry if they are available [slow; used by default]\n",
    "    ##                     no  - use the mean   geometry [fast]\n",
    "    mintpy.topographicResidual                   = no  #[yes / no], auto for yes\n",
    "    mintpy.topographicResidual.polyOrder         = auto  #[1-inf], auto for 2, poly order of temporal deformation model\n",
    "    mintpy.topographicResidual.phaseVelocity     = auto  #[yes / no], auto for no - use phase velocity for minimization\n",
    "    mintpy.topographicResidual.stepFuncDate      = auto  #[20080529,20190704T1733 / no], auto for no, date of step jump\n",
    "    mintpy.topographicResidual.excludeDate       = auto  #[20070321 / txtFile / no], auto for exclude_date.txt\n",
    "    mintpy.topographicResidual.pixelwiseGeometry = auto  #[yes / no], auto for yes, use pixel-wise geometry info\n",
    "\n",
    "\n",
    "    ########## 9.1 residual_RMS (root mean squares for noise evaluation)\n",
    "    ## Calculate the Root Mean Square (RMS) of residual phase time-series for each acquisition\n",
    "    ## reference: Yunjun et al. (2019, section 4.9 and 5.4)\n",
    "    ## To get rid of long spatial wavelength component, a ramp is removed for each acquisition\n",
    "    ## Set optimal reference date to date with min RMS\n",
    "    ## Set exclude dates (outliers) to dates with RMS > cutoff * median RMS (Median Absolute Deviation)\n",
    "    mintpy.residualRMS.maskFile = linear  #[file name / no], auto for maskTempCoh.h5, mask for ramp estimation\n",
    "    mintpy.residualRMS.deramp   = auto  #[quadratic / linear / no], auto for quadratic\n",
    "    mintpy.residualRMS.cutoff   = auto  #[0.0-inf], auto for 3\n",
    "\n",
    "    ########## 9.2 reference_date\n",
    "    ## Reference all time-series to one date in time\n",
    "    ## reference: Yunjun et al. (2019, section 4.9)\n",
    "    ## no     - do not change the default reference date (1st date)\n",
    "    mintpy.reference.date = no   #[reference_date.txt / 20090214 / no], auto for reference_date.txt\n",
    "\n",
    "\n",
    "    ########## 10. velocity\n",
    "    ## Estimate a suite of time functions [linear velocity by default]\n",
    "    ## from final displacement file (and from tropospheric delay file if exists)\n",
    "    mintpy.timeFunc.startDate   = auto   #[20070101 / no], auto for no\n",
    "    mintpy.timeFunc.endDate     = auto   #[20101230 / no], auto for no\n",
    "    mintpy.timeFunc.excludeDate = auto   #[exclude_date.txt / 20080520,20090817 / no], auto for exclude_date.txt\n",
    "\n",
    "    ## Fit a suite of time functions\n",
    "    ## reference: Hetland et al. (2012, JGR) equation (2-9)\n",
    "    ## polynomial function    is  defined by its degree in integer. 1 for linear, 2 for quadratic, etc.\n",
    "    ## periodic   function(s) are defined by a list of periods in decimal years. 1 for annual, 0.5 for semi-annual, etc.\n",
    "    ## step       function(s) are defined by a list of onset times in str in YYYYMMDD(THHMM) format\n",
    "    ## exp & log  function(s) are defined by an onset time followed by an charateristic time in integer days.\n",
    "    ##   Multiple exp and log functions can be overlaied on top of each other, achieved via e.g.:\n",
    "    ##   20110311,60,120          - two functions sharing the same onset time OR\n",
    "    ##   20110311,60;20170908,120 - separated by \";\"\n",
    "    mintpy.timeFunc.polynomial = auto   #[int >= 0], auto for 1, degree of the polynomial function\n",
    "    mintpy.timeFunc.periodic   = auto   #[1,0.5 / list_of_float / no], auto for no, periods in decimal years\n",
    "    mintpy.timeFunc.stepDate   = auto   #[20110311,20170908 / 20120928T1733 / no], auto for no, step function(s)\n",
    "    mintpy.timeFunc.exp        = auto   #[20110311,60 / 20110311,60,120 / 20110311,60;20170908,120 / no], auto for no\n",
    "    mintpy.timeFunc.log        = auto   #[20110311,60 / 20110311,60,120 / 20110311,60;20170908,120 / no], auto for no\n",
    "\n",
    "    ## Uncertainty quantification methods:\n",
    "    ## a. residue    - propagate from fitting residue assuming normal dist. in time (Fattahi & Amelung, 2015, JGR)\n",
    "    ## b. covariance - propagate from time series (co)variance matrix\n",
    "    ## c. bootstrap  - bootstrapping (independently resampling with replacement; Efron & Tibshirani, 1986, Stat. Sci.)\n",
    "    mintpy.timeFunc.uncertaintyQuantification = auto   #[residue, covariance, bootstrap], auto for residue\n",
    "    mintpy.timeFunc.timeSeriesCovFile         = auto   #[filename / no], auto for no, time series covariance file\n",
    "    mintpy.timeFunc.bootstrapCount            = auto   #[int>1], auto for 400, number of iterations for bootstrapping\n",
    "\n",
    "\n",
    "    ########## 11.1 geocode (post-processing)\n",
    "    # for input dataset in radar coordinates only\n",
    "    # commonly used resolution in meters and in degrees (on equator)\n",
    "    # 100,         90,          60,          50,          40,          30,          20,          10\n",
    "    # 0.000925926, 0.000833334, 0.000555556, 0.000462963, 0.000370370, 0.000277778, 0.000185185, 0.000092593\n",
    "    mintpy.geocode              = auto  #[yes / no], auto for yes\n",
    "    mintpy.geocode.SNWE         = auto  #[-1.2,0.5,-92,-91 / none ], auto for none, output extent in degree\n",
    "    mintpy.geocode.laloStep     = None  #[-0.000555556,0.000555556 / None], auto for None, output resolution in degree\n",
    "    mintpy.geocode.interpMethod = auto  #[linear], auto for nearest, interpolation method\n",
    "    mintpy.geocode.fillValue    = auto  #[np.nan, 0, ...], auto for np.nan, fill value for outliers.\n",
    "\n",
    "    ########## 11.2 google_earth (post-processing)\n",
    "    mintpy.save.kmz             = auto   #[yes / no], auto for yes, save geocoded velocity to Google Earth KMZ file\n",
    "\n",
    "    ########## 11.3 hdfeos5 (post-processing)\n",
    "    mintpy.save.hdfEos5         = auto   #[yes / no], auto for no, save time-series to HDF-EOS5 format\n",
    "    mintpy.save.hdfEos5.update  = auto   #[yes / no], auto for no, put XXXXXXXX as endDate in output filename\n",
    "    mintpy.save.hdfEos5.subset  = auto   #[yes / no], auto for no, put subset range info   in output filename\n",
    "\n",
    "    ########## 11.4 plot\n",
    "    # for high-resolution plotting, increase mintpy.plot.maxMemory\n",
    "    # for fast plotting with more parallelization, decrease mintpy.plot.maxMemory\n",
    "    mintpy.plot           = auto  #[yes / no], auto for yes, plot files generated by default processing to pic folder\n",
    "    mintpy.plot.dpi       = auto  #[int], auto for 150, number of dots per inch (DPI)\n",
    "    mintpy.plot.maxMemory = auto  #[float], auto for 4, max memory used by one call of view.py for plotting.\n",
    "    '''\n",
    "    \n",
    "    os.chdir(mintpy_path)\n",
    "    config_file = f'SenAT12.txt'\n",
    "    write_config_file(config_file, CONFIG_TXT, mode='w')\n",
    "    \n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep load_data\n",
    "    view.main('inputs/ifgramStack.h5 unwrapPhase-* -v -5 5 --zero-mask --noaxis --noverbose'.split())\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep modify_network\n",
    "    plot_network.main('inputs/ifgramStack.h5 -t smallbaselineApp.cfg --figsize 12 4'.split())\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep reference_point\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep quick_overview\n",
    "    plot.plot_num_triplet_with_nonzero_integer_ambiguity('numTriNonzeroIntAmbiguity.h5', disp_fig=True, fig_size=[14, 4])\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep correct_unwrap_error\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep invert_network\n",
    "    view.main('avgSpatialCoh.h5 --noverbose'.split())\n",
    "    view.main('temporalCoherence.h5 --noverbose'.split())\n",
    "    view.main('timeseries.h5 -v -100 100 --noaxis'.split())\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep deramp\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep correct_topography\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep residual_RMS\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep reference_date\n",
    "    !conda run -n mintpy smallbaselineApp.py SenAT12.txt --dostep velocity\n",
    "    view.main('velocity.h5 --noverbose --vlim -0.5 0.5 -c RdBu'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc06e0-c366-4018-b769-f845a7017207",
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_mintpy_path = 'asc_mintpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85fab9-7ad7-4863-aae2-fa79f480af7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run ascending time series\n",
    "os.chdir(proc_path)\n",
    "os.makedirs(asc_mintpy_path, exist_ok=True)\n",
    "\n",
    "run_mintpy(mintpy_path=asc_mintpy_path,\n",
    "           data_dir=data_path,\n",
    "           burst_name=asc_burst,\n",
    "           dam_name=dam_name,\n",
    "           reference_point=reference_point, \n",
    "           dam_gdf=dam_gdf,\n",
    "           use_offsets=use_offsets, \n",
    "           drop_acquisitions=asc_bad_acquisitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a9c48-5b89-4ca3-a0c1-07ce0d81a82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write velocity to geotiff\n",
    "\n",
    "# function to rewrite coordinates from metadata\n",
    "def coord_range(ds):\n",
    "    latrange = np.linspace(float(ds.attrs['Y_FIRST']),\n",
    "                           ((float(ds.attrs['Y_STEP'])*float(ds.attrs['LENGTH']))+float(ds.attrs['Y_FIRST'])),\n",
    "                           int(ds.attrs['LENGTH']))\n",
    "    lonrange = np.linspace(float(ds.attrs['X_FIRST']),\n",
    "                           ((float(ds.attrs['X_STEP'])*float(ds.attrs['WIDTH']))+float(ds.attrs['X_FIRST'])),\n",
    "                           int(ds.attrs['WIDTH']))\n",
    "    return latrange, lonrange\n",
    "\n",
    "def mintpy2d_to_xarray(fn, crs):\n",
    "    ds = xr.open_dataset(fn, engine='h5netcdf', phony_dims='sort')\n",
    "    ds = ds.rename_dims({'phony_dim_0':'y',\n",
    "                         'phony_dim_1':'x'\n",
    "                        })\n",
    "    latrange, lonrange = coord_range(ds)\n",
    "    ds = ds.assign_coords({'y': ('y', latrange),\n",
    "                          'x': ('x', lonrange)})\n",
    "    ds = ds.rio.write_crs(crs)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84affd1-d74d-474b-b581-db20a6198bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(proc_path)\n",
    "asc_vel_ds = mintpy2d_to_xarray(f'{asc_mintpy_path}/velocity.h5', crs)\n",
    "asc_vel_ds = asc_vel_ds.velocity.rio.write_nodata(0, inplace=True)\n",
    "asc_vel_ds = asc_vel_ds * -1 # keep normal insar sign convention\n",
    "asc_vel_ds.rio.to_raster(f'{asc_mintpy_path}/asc_mean_velocity_{dam_name}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ec3ca-a253-4745-87e0-59c36aa3ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "des_mintpy_path = 'des_mintpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bce9f35-89a4-40d3-b8a8-61eb88773406",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# run descending time series\n",
    "os.chdir(proc_path)\n",
    "os.makedirs(des_mintpy_path, exist_ok=True)\n",
    "\n",
    "run_mintpy(mintpy_path=des_mintpy_path,\n",
    "           data_dir=data_path,\n",
    "           burst_name=des_burst,\n",
    "           dam_name=dam_name,\n",
    "           reference_point=reference_point, \n",
    "           dam_gdf=dam_gdf,\n",
    "           use_offsets=use_offsets, \n",
    "           drop_acquisitions=des_bad_acquisitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993c5b2-78ad-43d5-82e7-339b1d2110e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(proc_path)\n",
    "des_vel_ds = mintpy2d_to_xarray(f'{des_mintpy_path}/velocity.h5', crs)\n",
    "des_vel_ds = des_vel_ds.velocity.rio.write_nodata(0, inplace=True)\n",
    "des_vel_ds = des_vel_ds * -1 # keep normal insar sign convention\n",
    "des_vel_ds.rio.to_raster(f'{des_mintpy_path}/des_mean_velocity_{dam_name}.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e49ef-5297-4dbf-b334-3d6d82373c36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "des_vel_ds.plot(ax=ax[0], vmin=-0.4, vmax=0.4, cmap='RdBu_r')\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].set_title('descending velocity')\n",
    "ax[0].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[0].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "asc_vel_ds.plot(ax=ax[1], vmin=-0.4, vmax=0.4, cmap='RdBu_r')\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[1].set_title('ascending velocity')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "ax[1].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[1].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b4be1-aa90-4be6-8167-890c601eb038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## decompose into EW and UD components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32361cc3-41c5-4cc4-bb45-695bcfa65fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to read time series into xarray\n",
    "def mintpyTS_to_xarray(fn, crs):\n",
    "    ds = xr.open_dataset(fn, cache=False)\n",
    "    ds = ds.rename_dims({'phony_dim_1':'time',\n",
    "                         'phony_dim_2':'y',\n",
    "                         'phony_dim_3':'x'})\n",
    "    ds = ds.rename({'timeseries': 'displacement'})\n",
    "    latrange, lonrange = coord_range(ds)\n",
    "    ds = ds.assign_coords({'time': ('time', pd.to_datetime(ds.date)),\n",
    "                           'y': ('y', latrange),\n",
    "                           'x': ('x', lonrange)})\n",
    "    ds = ds.drop(['bperp', 'date'])\n",
    "    ds = ds.rio.write_crs(crs)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98203e6-9b0a-4d31-8646-7170306238fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(proc_path)\n",
    "# read ascending and descending time series into xarray ds\n",
    "asc_ts_fn = f'{asc_mintpy_path}/timeseries.h5'\n",
    "asc_ts_ds = mintpyTS_to_xarray(asc_ts_fn, crs)\n",
    "asc_ts_ds = asc_ts_ds.rename({'displacement': 'asc_displacement'})\n",
    "\n",
    "des_ts_fn = f'{des_mintpy_path}/timeseries.h5'\n",
    "des_ts_ds = mintpyTS_to_xarray(des_ts_fn, crs)\n",
    "des_ts_ds = des_ts_ds.rename({'displacement': 'des_displacement'})\n",
    "\n",
    "# read geometry files into xarray ds\n",
    "asc_geom_fn = f'{asc_mintpy_path}/inputs/geometryGeo.h5'\n",
    "des_geom_fn = f'{des_mintpy_path}/inputs/geometryGeo.h5'\n",
    "asc_geom_ds = mintpy2d_to_xarray(asc_geom_fn, crs)\n",
    "des_geom_ds = mintpy2d_to_xarray(des_geom_fn, crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6166d-5cb6-4349-901d-9542e68f6882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find shorter time series and interpolate\n",
    "asc_tspan = asc_ts_ds.time[-1] - asc_ts_ds.time[0]\n",
    "des_tspan = des_ts_ds.time[-1] - des_ts_ds.time[0]\n",
    "\n",
    "if des_tspan > asc_tspan:\n",
    "    ts_ds = asc_ts_ds.assign(des_displacement=des_ts_ds.interp(time=asc_ts_ds.time).des_displacement)\n",
    "    ts_ds['des_displacement'] = ts_ds['des_displacement'] - ts_ds['des_displacement'].isel(time=0)\n",
    "elif asc_tspan > des_tspan:\n",
    "    ts_ds = des_ts_ds.assign(asc_displacement=asc_ts_ds.interp(time=des_ts_ds.time).asc_displacement)\n",
    "    ts_ds['asc_displacement'] = ts_ds['asc_displacement'] - ts_ds['asc_displacement'].isel(time=0)\n",
    "\n",
    "else:\n",
    "    print('same time span, interpolate to ascending')\n",
    "    ts_ds = asc_ts_ds.assign(des_displacement=des_ts_ds.interp(time=asc_ts_ds.time).des_displacement)\n",
    "    ts_ds['des_displacement'] = ts_ds['des_displacement'] - ts_ds['des_displacement'].isel(time=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46367f-59fb-4508-b5e4-c495530eb8a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot time series for an arbitrary date\n",
    "time_index = 20\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ts_ds.asc_displacement.isel(time=time_index).plot(ax=ax[0], cmap='RdBu', vmin=-0.05, vmax=0.05)\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].set_title('ascending displacement')\n",
    "ax[0].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[0].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "ts_ds.des_displacement.isel(time=time_index).plot(ax=ax[1], cmap='RdBu', vmin=-0.05, vmax=0.05)\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[1].set_title('descending displacement')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "ax[1].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[1].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c83cc-d1d4-4e02-a31e-ba7952500dce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define azimuth angle from sensor to target relative to north\n",
    "asc_geom_ds['azimuthAngle'] =  90 - (asc_geom_ds['azimuthAngle'] - 90)\n",
    "des_geom_ds['azimuthAngle'] = np.abs(des_geom_ds['azimuthAngle'] + 90) + 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61255943-ce0b-4b77-b7cd-a0773a51c69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot ascending and descending azimuth and incidence angles\n",
    "f, ax = plt.subplots(2, 2, figsize=(10, 6), sharex=True, sharey=True)\n",
    "asc_geom_ds.azimuthAngle.plot(ax=ax[0,0], cbar_kwargs={'shrink': 0.5})\n",
    "asc_geom_ds.incidenceAngle.plot(ax=ax[0,1], cbar_kwargs={'shrink': 0.5})\n",
    "des_geom_ds.azimuthAngle.plot(ax=ax[1,0], cbar_kwargs={'shrink': 0.5})\n",
    "des_geom_ds.incidenceAngle.plot(ax=ax[1,1], cbar_kwargs={'shrink': 0.5})\n",
    "ax[0, 0].set_aspect('equal')\n",
    "ax[0, 1].set_aspect('equal')\n",
    "ax[1, 0].set_aspect('equal')\n",
    "ax[1, 1].set_aspect('equal')\n",
    "ax[0, 0].set_title('ascending azimuth angle')\n",
    "ax[0, 1].set_title('ascending incidence angle')\n",
    "ax[1, 0].set_title('descending azimuth angle')\n",
    "ax[1, 1].set_title('descending incidence angle')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279beb9e-1efd-4e75-8275-63b18aabbb86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct unit look vectors \n",
    "asc_geom_ds['n_hat'] = np.cos(np.radians(asc_geom_ds.azimuthAngle))*np.sin(np.radians(asc_geom_ds.incidenceAngle))\n",
    "asc_geom_ds['e_hat'] = np.sin(np.radians(asc_geom_ds.azimuthAngle))*np.sin(np.radians(asc_geom_ds.incidenceAngle))\n",
    "asc_geom_ds['z_hat'] = np.cos(np.radians(asc_geom_ds.incidenceAngle))\n",
    "\n",
    "des_geom_ds['n_hat'] = np.cos(np.radians(des_geom_ds.azimuthAngle))*np.sin(np.radians(des_geom_ds.incidenceAngle))\n",
    "des_geom_ds['e_hat'] = np.sin(np.radians(des_geom_ds.azimuthAngle))*np.sin(np.radians(des_geom_ds.incidenceAngle))\n",
    "des_geom_ds['z_hat'] = np.cos(np.radians(des_geom_ds.incidenceAngle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d0589-f27f-4998-8358-ace04013d85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot unit look vectors \n",
    "f, ax = plt.subplots(2, 3, figsize=(12, 5), sharex=True, sharey=True)\n",
    "asc_geom_ds.n_hat.plot(ax=ax[0, 0], cbar_kwargs={'shrink': 0.5})\n",
    "asc_geom_ds.e_hat.plot(ax=ax[0, 1], cbar_kwargs={'shrink': 0.5})\n",
    "asc_geom_ds.z_hat.plot(ax=ax[0, 2], cbar_kwargs={'shrink': 0.5})\n",
    "\n",
    "des_geom_ds.n_hat.plot(ax=ax[1, 0], cbar_kwargs={'shrink': 0.5})\n",
    "des_geom_ds.e_hat.plot(ax=ax[1, 1], cbar_kwargs={'shrink': 0.5})\n",
    "des_geom_ds.z_hat.plot(ax=ax[1, 2], cbar_kwargs={'shrink': 0.5})\n",
    "ax[0, 0].set_aspect('equal')\n",
    "ax[0, 1].set_aspect('equal')\n",
    "ax[0, 2].set_aspect('equal')\n",
    "ax[1, 0].set_aspect('equal')\n",
    "ax[1, 1].set_aspect('equal')\n",
    "ax[1, 2].set_aspect('equal')\n",
    "\n",
    "ax[0, 0].set_title('ascending n hat')\n",
    "ax[0, 1].set_title('ascending e hat')\n",
    "ax[0, 2].set_title('ascending z hat')\n",
    "ax[1, 0].set_title('descending n hat')\n",
    "ax[1, 1].set_title('descending e hat')\n",
    "ax[1, 2].set_title('descending z hat')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d20f4-82a9-4b79-bb39-b3b80de37b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct ud and ew unit vectors [n, e, d]\n",
    "vertical_unit_vector = np.array([np.zeros_like(des_geom_ds['n_hat'].values), np.zeros_like(des_geom_ds['n_hat'].values), np.ones_like(des_geom_ds['n_hat'].values)])\n",
    "horizontal_unit_vector = np.array([np.zeros_like(des_geom_ds['n_hat'].values), np.ones_like(des_geom_ds['n_hat'].values), np.zeros_like(des_geom_ds['n_hat'].values)])\n",
    "# north\n",
    "#horizontal_unit_vector = np.array([np.ones_like(des_geom_ds['n_hat'].values), np.zeros_like(des_geom_ds['n_hat'].values), np.zeros_like(des_geom_ds['n_hat'].values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1ea29-2192-4aed-a04f-05e9092e015e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find dot products for LOS\n",
    "asc_vertical = np.einsum('ijk,ijk->jk', np.array([asc_geom_ds['n_hat'].values, asc_geom_ds['e_hat'].values, asc_geom_ds['z_hat'].values]), vertical_unit_vector)\n",
    "asc_horizontal = np.einsum('ijk,ijk->jk', np.array([asc_geom_ds['n_hat'].values, asc_geom_ds['e_hat'].values, asc_geom_ds['z_hat'].values]), horizontal_unit_vector)\n",
    "des_vertical = np.einsum('ijk,ijk->jk', np.array([des_geom_ds['n_hat'].values, des_geom_ds['e_hat'].values, des_geom_ds['z_hat'].values]), vertical_unit_vector)\n",
    "des_horizontal = np.einsum('ijk,ijk->jk', np.array([des_geom_ds['n_hat'].values, des_geom_ds['e_hat'].values, des_geom_ds['z_hat'].values]), horizontal_unit_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0489d33c-42fe-437d-a4bd-da6378ea590d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solve for horizontal and vertical displacement for all dates \n",
    "time_index = 20\n",
    "ts_ds['vertical_displacement'] = (asc_horizontal*ts_ds.des_displacement - des_horizontal*ts_ds.asc_displacement)/(asc_horizontal*des_vertical - des_horizontal*asc_vertical)\n",
    "ts_ds['horizontal_displacement'] = ((asc_vertical*ts_ds['vertical_displacement']) - ts_ds.asc_displacement)/-asc_horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26d2af-bdcb-4bee-9e8c-653089767274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot cumulative ew and ud displacement for an arbitrary date\n",
    "time_index = 80\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ts_ds.isel(time=time_index).vertical_displacement.plot(ax=ax[0], cmap='RdBu', vmin=-2, vmax=2)\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[0].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "ts_ds.isel(time=time_index).horizontal_displacement.plot(ax=ax[1], cmap='BrBG_r', vmin=-2, vmax=2)\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "ax[1].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[1].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265ee99d-c58d-4fce-9a72-5d83a79dafe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "decimal_years = ((ts_ds['time'] - np.datetime64('1970-01-01T00:00:00'))/np.timedelta64(1, 'ns')/1e9/3600/24/365.25)+1970\n",
    "ts_ds = ts_ds.assign_coords(decimal_year = ('time', decimal_years.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909a023-a95e-4675-b23d-f077cc715873",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to fit a line to a 1D array and return the slope\n",
    "def fit_line(x, y):\n",
    "    valid_indices = ~np.isnan(y)\n",
    "    try: \n",
    "        slope, _, _, _, _ = linregress(x[valid_indices], y[valid_indices])\n",
    "    except: \n",
    "        return np.nan\n",
    "    else:\n",
    "        return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f858616-287d-4c82-95ae-7c46561414c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertical_veloc = xr.apply_ufunc(fit_line, ts_ds.decimal_year, ts_ds.vertical_displacement, input_core_dims=[['time'], ['time']], vectorize=True)\n",
    "horizontal_veloc = xr.apply_ufunc(fit_line, ts_ds.decimal_year, ts_ds.horizontal_displacement, input_core_dims=[['time'], ['time']], vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be8ddd-b3fc-4ec5-bb3a-e60a37657661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot ew and ud velocity\n",
    "f, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "vertical_veloc.plot(ax=ax[0], cmap='RdBu', vmin=-0.1, vmax=0.1)\n",
    "dam_gdf.plot(ax=ax[0], edgecolor='black', facecolor='none')\n",
    "ax[0].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[0].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "horizontal_veloc.plot(ax=ax[1], cmap='BrBG_r', vmin=-0.1, vmax=0.1)\n",
    "dam_gdf.plot(ax=ax[1], edgecolor='black', facecolor='none')\n",
    "ax[1].set_xlim(aoi_extent[0]+2000, aoi_extent[2]-2000)\n",
    "ax[1].set_ylim(aoi_extent[3]+2000, aoi_extent[1]-2000)\n",
    "ax[0].set_title('vertical velocity (m/yr)')\n",
    "ax[1].set_title('e/w velocity (m/yr)')\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "f.tight_layout()\n",
    "plt.savefig('./figs/vertical_ew_velocity.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d876e847-8de0-40e3-ad35-3d939d199cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write to rasters\n",
    "vertical_veloc = vertical_veloc.rio.write_crs(crs)\n",
    "vertical_veloc.rio.to_raster(f'vertical_velocity_{dam_name}.tif')\n",
    "horizontal_veloc = horizontal_veloc.rio.write_crs(crs)\n",
    "horizontal_veloc.rio.to_raster(f'ew_velocity_{dam_name}.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79189b8-eefd-41fe-883e-01a1e5e58938",
   "metadata": {
    "tags": []
   },
   "source": [
    "## spatial analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ef772-dd0b-4416-aca4-3a03ddd53642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load in moving area polygon\n",
    "moving_fn = f'../../mapping/polygons/{dam_name}_moving.shp'\n",
    "moving_gdf = gpd.read_file(moving_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd344a0-c00f-4407-bdfc-44562d8585d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moving_ts = ts_ds.rio.clip(moving_gdf.geometry, crs=moving_gdf.crs, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c8f56-980b-494e-9fb5-d8c3dfc34ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs('figs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a992f4a-daae-4517-a192-7653cee5ab73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ascending and descending only\n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(figsize=(7,3))\n",
    "ax.axhline(y=0, c='gray', alpha=0.5)\n",
    "(moving_ts.asc_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax, c='purple', marker='o', markersize=2, linewidth=1, alpha=0.7, label='ascending')\n",
    "(moving_ts.des_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax, c='indianred', marker='o', markersize=2, linewidth=1, alpha=0.7, label='descending')\n",
    "ax.set_ylabel('displacement (cm)')\n",
    "ax.set_title('median displacement')\n",
    "f.tight_layout()\n",
    "f.legend()\n",
    "plt.savefig(f'./figs/{dam_name}_los_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fc04b-4cac-412a-8e1b-43d201074261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(9,5), sharex=True, sharey=True)\n",
    "ax[0].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts.x)):\n",
    "    for y in range(len(moving_ts.y)):\n",
    "        if (x+1) % 3 + (y+1) % 3 == 0:\n",
    "            (moving_ts.des_displacement.isel(x=x, y=y)*100).plot(ax=ax[0], c='red', linewidth=1, alpha=0.2)\n",
    "(moving_ts.des_displacement.median(dim=('x', 'y'))*100).plot(ax=ax[0], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[1].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts.x)):\n",
    "    for y in range(len(moving_ts.y)):\n",
    "        if (x+1) % 3 + (y+1) % 3 == 0:\n",
    "            (moving_ts.asc_displacement.isel(x=x, y=y)*100).plot(ax=ax[1], c='orchid', linewidth=1, alpha=0.2)\n",
    "(moving_ts.asc_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax[1], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[0].set_title('des cumulative displacement')\n",
    "ax[0].set_ylabel('displacement (cm)')\n",
    "ax[1].set_title('asc cumulative displacement')\n",
    "ax[1].set_ylabel('displacement (cm)')\n",
    "f.tight_layout()\n",
    "plt.savefig(f'./figs/{dam_name}_los_pixels_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5824a-a667-4b27-9405-68f2a45c417d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ew and up/down only\n",
    "sns.set_theme()\n",
    "f, ax = plt.subplots(figsize=(7,3))\n",
    "ax.axhline(y=0, c='gray', alpha=0.5)\n",
    "(moving_ts.vertical_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax, c='indianred', marker='o', markersize=2, linewidth=1, alpha=0.7, label='vertical')\n",
    "(moving_ts.horizontal_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax, c='orchid', marker='o', markersize=2, linewidth=1, alpha=0.7, label='horizontal')\n",
    "ax.set_ylabel('displacement (cm)')\n",
    "ax.set_title('median displacement')\n",
    "f.tight_layout()\n",
    "f.legend()\n",
    "plt.savefig(f'./figs/{dam_name}_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c49e57-8441-46a7-b65b-b9ca7523a87f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(9,5), sharex=True, sharey=True)\n",
    "ax[0].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts.x)):\n",
    "    for y in range(len(moving_ts.y)):\n",
    "        if (x+1) % 3 + (y+1) % 3 == 0:\n",
    "            (moving_ts.vertical_displacement.isel(x=x, y=y)*100).plot(ax=ax[0], c='red', linewidth=1, alpha=0.2)\n",
    "(moving_ts.vertical_displacement.median(dim=('x', 'y'))*100).plot(ax=ax[0], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[1].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts.x)):\n",
    "    for y in range(len(moving_ts.y)):\n",
    "        if (x+1) % 3 + (y+1) % 3 == 0:\n",
    "            (moving_ts.horizontal_displacement.isel(x=x, y=y)*100).plot(ax=ax[1], c='orchid', linewidth=1, alpha=0.2)\n",
    "(moving_ts.horizontal_displacement.mean(dim=('x', 'y'))*100).plot(ax=ax[1], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[0].set_title('vertical cumulative displacement')\n",
    "ax[0].set_ylabel('displacement (cm)')\n",
    "ax[1].set_title('east/west cumulative displacement')\n",
    "ax[1].set_ylabel('displacement (cm)')\n",
    "f.tight_layout()\n",
    "plt.savefig(f'./figs/{dam_name}_decomp_pixels_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae91908-e64f-44d5-8d85-961641936845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vertical and ew displacement for an arbitrary time step\n",
    "time_index = -2\n",
    "f, ax = plt.subplots()\n",
    "ax.scatter((horizontal_veloc.rio.clip(moving_gdf.geometry, crs=moving_gdf.crs, drop=True).values.ravel()*100),\n",
    "           (vertical_veloc.rio.clip(moving_gdf.geometry, crs=moving_gdf.crs, drop=True).values.ravel()*100),\n",
    "           alpha=0.3)\n",
    "ax.set_xlabel('east/west velocity (cm/yr)')\n",
    "ax.set_ylabel('vertical velocity (cm/yr)')\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('direction and magnitude of moving area pixels')\n",
    "plt.savefig(f'./figs/{dam_name}_scatter_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65018f22-05fc-43ac-8a7e-87f477dd28d9",
   "metadata": {},
   "source": [
    "## calculate instantaneous velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a454a9f-b48a-4d4f-9beb-d3cd400e0b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# interpolate in time\n",
    "moving_ts_weekly = moving_ts.resample(time='2W').interpolate('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c858cd-f17c-43e0-b04c-66b29a557702",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moving_ts_weekly['vertical_velocity'] = moving_ts_weekly.vertical_displacement.differentiate(coord='time', edge_order=1, datetime_unit='D')*365.25\n",
    "moving_ts_weekly['horizontal_velocity'] = moving_ts_weekly.horizontal_displacement.differentiate(coord='time', edge_order=1, datetime_unit='D')*365.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e47794-c035-4d6a-8dea-d92327e3dcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(9,6), sharex=True, sharey=True)\n",
    "ax[0].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts_weekly.x)):\n",
    "    for y in range(len(moving_ts_weekly.y)):\n",
    "        if (x+1) % 10 + (y+1) % 10 == 0:\n",
    "            (moving_ts_weekly.vertical_velocity.isel(x=x, y=y)*100).plot(ax=ax[0], c='red', linewidth=1, alpha=0.2)\n",
    "(moving_ts_weekly.vertical_velocity.mean(dim=('x', 'y'))*100).plot(ax=ax[0], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[1].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_ts_weekly.x)):\n",
    "    for y in range(len(moving_ts_weekly.y)):\n",
    "        if (x+1) % 10 + (y+1) % 10 == 0:\n",
    "            (moving_ts_weekly.horizontal_velocity.isel(x=x, y=y)*100).plot(ax=ax[1], c='orchid', linewidth=1, alpha=0.2)\n",
    "(moving_ts_weekly.horizontal_velocity.mean(dim=('x', 'y'))*100).plot(ax=ax[1], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "# ax[0].set_ylim(-20, 20)\n",
    "# ax[1].set_ylim(-20, 20)\n",
    "\n",
    "ax[0].set_title('vertical velocity')\n",
    "ax[0].set_ylabel('velocity (cm/yr)')\n",
    "ax[1].set_title('east/west velocity')\n",
    "ax[1].set_ylabel('velocity (cm/yr)')\n",
    "f.tight_layout()\n",
    "#plt.savefig(f'./figs/{dam_name}_decomp_pixels_ts.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65cb845-43fa-4b6b-8755-33d6ca788a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "moving_months = moving_ts_weekly.groupby(moving_ts_weekly.time.dt.month).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c4fdd-46b9-4002-a3c5-b64a903b1b7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(9,6), sharex=True, sharey=False)\n",
    "ax[0].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_months.x)):\n",
    "    for y in range(len(moving_months.y)):\n",
    "        if (x+1) % 8 + (y+1) % 8 == 0:\n",
    "            (moving_months.vertical_velocity.isel(x=x, y=y)*100).plot(ax=ax[0], c='red', linewidth=1, alpha=0.2)\n",
    "(moving_months.vertical_velocity.median(dim=('x', 'y'))*100).plot(ax=ax[0], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[1].axhline(y=0, c='gray', alpha=0.5)\n",
    "for x in range(len(moving_months.x)):\n",
    "    for y in range(len(moving_months.y)):\n",
    "        if (x+1) % 8 + (y+1) % 8 == 0:\n",
    "            (moving_months.horizontal_velocity.isel(x=x, y=y)*100).plot(ax=ax[1], c='orchid', linewidth=1, alpha=0.2)\n",
    "(moving_months.horizontal_velocity.median(dim=('x', 'y'))*100).plot(ax=ax[1], c='k', linewidth=1, alpha=0.7)\n",
    "\n",
    "ax[0].set_title('vertical velocity')\n",
    "ax[0].set_ylabel('velocity (cm/yr)')\n",
    "ax[1].set_title('east/west velocity')\n",
    "ax[1].set_ylabel('velocity (cm/yr)')\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93462d38-19d3-437a-a36a-a0b5fce2d023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ones = xr.ones_like(moving_months.vertical_velocity)\n",
    "months_broadcast = ones*moving_months.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090cce7-b951-443e-9e10-982b9478a28d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(9, 6), sharex=True, sharey=True)\n",
    "ax[0].axhline(y=0, c='gray', alpha=1)\n",
    "ax[1].axhline(y=0, c='gray', alpha=1)\n",
    "sns.histplot(ax=ax[0], x=months_broadcast.values.flatten(), y = moving_months.vertical_velocity.values.flatten()*100, cbar=True, discrete=(True, False), binwidth=1, vmax=100, cmap='magma_r', alpha=0.8, cbar_kws={'label':'pixel count'})\n",
    "sns.histplot(ax=ax[1], x=months_broadcast.values.flatten(), y = moving_months.horizontal_velocity.values.flatten()*100, cbar=True, discrete=(True, False), binwidth=1, vmax=100, cmap='magma_r', alpha=0.8, cbar_kws={'label':'pixel count'})\n",
    "ax[0].set_xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "ax[1].set_xticks([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    "ax[0].set_xlim(0, 12.5)\n",
    "ax[1].set_xlim(0, 12.5)\n",
    "ax[0].set_ylabel('mean velocity over all years (cm/yr)')\n",
    "ax[0].set_xlabel('month')\n",
    "ax[1].set_xlabel('month')\n",
    "ax[0].set_title('vertical velocity')\n",
    "ax[1].set_title('east/west velocity')\n",
    "f.tight_layout()\n",
    "plt.savefig(f'./figs/{dam_name}_velocity.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mintpy",
   "language": "python",
   "name": "mintpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
